# -*- coding: utf-8 -*-
"""final_cost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9iEld2LNtZd4bv4e1XhCWjZGMnt8sKA
"""

# -*- coding: utf-8 -*-
"""Enhanced Falcon 9 Launch Cost Estimation"""

# ‚úÖ STEP 1: Install required packages
!pip install pandas numpy matplotlib seaborn scikit-learn shap joblib xgboost lime --quiet

# ‚úÖ STEP 2: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import joblib
import lime
import lime.lime_tabular

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ‚úÖ STEP 3: Load the dataset
df = pd.read_csv("dataset3_with_cost.csv")

# ‚úÖ STEP 4: Preprocess data
if 'EstimatedLaunchCost_MillionUSD' in df.columns:
    df = df.rename(columns={'EstimatedLaunchCost_MillionUSD': 'LaunchCost'})

columns_to_drop = ['Date', 'Serial']
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')
df = df.dropna()

# Convert any boolean columns to string (if they exist)
for col in df.select_dtypes(include='bool').columns:
    df[col] = df[col].astype(str)

# One-hot encode all object-type columns
categorical_cols = df.select_dtypes(include='object').columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Check for any non-numeric columns
non_numeric = df.select_dtypes(include=['object', 'bool']).columns
print("‚ö†Ô∏è Non-numeric columns remaining:", list(non_numeric))



# Visualize correlations
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Feature Correlation Matrix")
plt.tight_layout()
plt.show()

# ‚úÖ STEP 5: Define features and target
X = df.drop('LaunchCost', axis=1)
y = df['LaunchCost']

# ‚úÖ STEP 6: Train-test split and scaling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ‚úÖ STEP 7: Hyperparameter tuning
# SVR tuning
tuned_svr = GridSearchCV(SVR(), {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}, cv=3, scoring='neg_mean_squared_error')
tuned_svr.fit(X_train_scaled, y_train)

# XGBoost tuning
tuned_xgb = GridSearchCV(XGBRegressor(random_state=42), {
    'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.05, 0.1]
}, cv=3, scoring='neg_mean_squared_error')
tuned_xgb.fit(X_train_scaled, y_train)

# ‚úÖ STEP 8: Train models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'XGBoost': tuned_xgb.best_estimator_,
    'SVR': tuned_svr.best_estimator_
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    cv_r2 = cross_val_score(model, X_train_scaled, y_train, scoring='r2', cv=5).mean()
    results[name] = {
        'R2': r2_score(y_test, y_pred),
        'CV_R2': cv_r2,
        'MAE': mean_absolute_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
    }

results_df = pd.DataFrame(results).T
print("\nüìä Model Performance Comparison:")
print(results_df)

# ‚úÖ STEP 9: SHAP Explainability (Bar Plot)
best_model = models['Random Forest']
explainer = shap.Explainer(best_model, X_train_scaled)
shap_values = explainer(X_test_scaled)

# SHAP summary bar plot
shap.plots.bar(shap_values, max_display=10)  # shows top 10 features

# ‚úÖ STEP 10: Feature Importance Visualization
importances = best_model.feature_importances_
features = X.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=features[indices])
plt.title("üöÄ Feature Importances for Launch Cost Prediction")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# ‚úÖ STEP 11: LIME Explainability
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train_scaled,
    feature_names=features,
    class_names=['LaunchCost'],
    mode='regression'
)

lime_exp = lime_explainer.explain_instance(X_test_scaled[0], best_model.predict, num_features=10)
lime_exp.save_to_file("lime_explanation_cost.html")
print("\n‚úÖ LIME explanation saved to 'lime_explanation_cost.html'")

# ‚úÖ STEP 12: Save model and scaler
joblib.dump(best_model, "falcon9_cost_model.pkl")
joblib.dump(scaler, "cost_scaler.pkl")
print("\n‚úÖ Model and scaler saved successfully!")